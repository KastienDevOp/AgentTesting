"""\nExample usage of the Chalice Forged provider-agnostic client\n"""\nimport os\nfrom chalice_forged import create_client, ConfigManager, ProviderFactory\n\n\ndef demo_basic_usage():\n    """Demo basic usage with different providers"""\n    print("=== Chalice Forged Provider-Agnostic Demo ===\n")\n    \n    # Show available providers\n    available_providers = ProviderFactory.get_available_providers()\n    print(f"Available providers: {', '.join(available_providers)}\n")\n    \n    # Demo with Ollama (local, no API key needed)\n    print("1. Testing Ollama (local provider)...")\n    try:\n        client = create_client("ollama", base_url="http://localhost:11434", model="llama2")\n        \n        messages = [{\"role\": \"user\", \"content\": \"Hello! Can you tell me about yourself in one sentence?\"}]\n        response = client.chat(messages)\n        \n        if response.get(\"success\"):\n            print(f"✅ Ollama response: {response['content'][:100]}...")\n        else:\n            print(f"❌ Ollama error: {response.get('error', 'Unknown error')}")\n    except Exception as e:\n        print(f"❌ Ollama connection failed: {e}")\n    \n    print()\n    \n    # Demo configuration management\n    print("2. Configuration Management Demo...")\n    config_manager = ConfigManager()\n    \n    # Show configured providers\n    configured = config_manager.get_available_providers()\n    print(f"Configured providers: {', '.join(configured) if configured else 'None'}")\n    \n    # Show example config\n    print("\nExample configuration structure:")\n    example_config = config_manager.create_example_config()\n    for provider, config in list(example_config.items())[:2]:  # Show first 2\n        print(f"  {provider}: {config}")\n    \n    print("\n3. Provider Switching Demo...")\n    # Demo switching providers (without actual API calls due to no keys)\n    try:\n        client = create_client("ollama", model="llama2")\n        print(f"✅ Created client with provider: {client.provider_name}")\n        \n        # Get available models (works with Ollama if running)\n        try:\n            models = client.get_models()\n            print(f"Available models: {models[:3]}..." if len(models) > 3 else f"Available models: {models}")\n        except:\n            print("Could not fetch models (Ollama may not be running)")\n            \n    except Exception as e:\n        print(f"❌ Provider demo failed: {e}")\n\n\ndef demo_advanced_features():\n    """Demo advanced features like streaming and different completion types"""\n    print("\n=== Advanced Features Demo ===\n")\n    \n    try:\n        client = create_client("ollama", model="llama2")\n        \n        # Demo different completion types\n        print("1. Text completion...")\n        text_response = client.complete("The future of AI is")\n        if text_response.get(\"success\"):\n            print(f"✅ Text completion: {text_response['content'][:50]}...")\n        \n        print("\n2. Chat completion...")\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is Python?\"}\n        ]\n        chat_response = client.chat(messages)\n        if chat_response.get(\"success\"):\n            print(f"✅ Chat completion: {chat_response['content'][:50]}...")\n        \n        print("\n3. Streaming demo...")\n        print("Streaming response: ", end="")\n        for chunk in client.stream_chat(messages):\n            if chunk.get(\"success\") and chunk.get(\"content\"):\n                print(chunk[\"content\"], end=\"\", flush=True)\n                break  # Just show first chunk for demo\n        print("\n(streaming truncated for demo)")\n        \n    except Exception as e:\n        print(f"❌ Advanced features demo failed: {e}")\n\n\nif __name__ == \"__main__\":\n    demo_basic_usage()\n    demo_advanced_features()\n    \n    print("\n=== Setup Instructions ===")\n    print("To use with API providers, set environment variables:")\n    print("  export OPENAI_API_KEY='your-openai-key'")\n    print("  export ANTHROPIC_API_KEY='your-anthropic-key'")\n    print("  export GOOGLE_API_KEY='your-google-key'")\n    print("  # ... etc for other providers")\n    print("\nOr create a .env file with your API keys.")\n    print("\nFor local usage, install and run Ollama:")\n    print("  # Install Ollama from https://ollama.ai")\n    print("  ollama run llama2")